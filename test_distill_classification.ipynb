{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from GPT2 import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' #'cuda'\n",
    "\n",
    "model = GPT2Model(\n",
    "    vocab_size=30000,\n",
    "    layer_size=12,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=768,\n",
    "    num_attention_heads=12,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)\n",
    "\n",
    "state_dict = torch.load('save_distill.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    'GPT2/bpe/vocab.json',\n",
    "    'GPT2/bpe/chinese_vocab.model',\n",
    "    max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(text, max_len=10):\n",
    "    ids = tokenizer.encode(text)\n",
    "    input_id = torch.tensor((np.array(ids).reshape(1, -1).astype('int64'))).to(device)\n",
    "    output, cached_kvs = model(input_id, use_cache=True)\n",
    "    nid = int(np.argmax(output[0, -1].detach().cpu().numpy()))\n",
    "    ids += [nid]\n",
    "    out = [nid]\n",
    "    for i in range(max_len):\n",
    "        input_id = torch.tensor(np.array([nid]).reshape(1, -1).astype('int64')).to(device)\n",
    "        output, cached_kvs = model(input_id, cached_kvs, use_cache=True)\n",
    "        nid = int(np.argmax(output[0, -1].detach().cpu().numpy()))\n",
    "        ids += [nid]\n",
    "        if nid==3:\n",
    "            break\n",
    "        out.append(nid)\n",
    "    print(tokenizer.decode(out))\n",
    "\n",
    "def ask_question(question, max_len=10):\n",
    "    sample('''问题：中国的首都是哪里？\n",
    "    答案：北京。\n",
    "    问题：李白在哪个朝代？\n",
    "    答案：唐朝。\n",
    "    问题：%s\n",
    "    答案：''' % question, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.569 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "白羊座。\n"
     ]
    }
   ],
   "source": [
    "ask_question('为什么会被白羊座？', max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from data.samplers import DistributedBatchSampler, RandomSampler\n",
    "\n",
    "\n",
    "def get_masks_and_position_ids(data,\n",
    "                               eod_token,\n",
    "                               reset_position_ids,\n",
    "                               reset_attention_mask):\n",
    "    # Extract batch size and sequence length.\n",
    "    batch_size, seq_length = data.size()\n",
    "\n",
    "    # Attention mask (lower triangular).\n",
    "    if reset_attention_mask:\n",
    "        att_mask_batch = batch_size\n",
    "    else:\n",
    "        att_mask_batch = 1\n",
    "    attention_mask = torch.tril(torch.ones(\n",
    "        (att_mask_batch, seq_length, seq_length), device=data.device)).view(att_mask_batch, 1, seq_length, seq_length)\n",
    "\n",
    "    # Loss mask.\n",
    "    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)\n",
    "    loss_mask[data == eod_token] = 0.0\n",
    "\n",
    "    # Position ids.\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long,\n",
    "                                device=data.device)\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(data)\n",
    "    # We need to clone as the ids will be modifed based on batch index.\n",
    "    if reset_position_ids:\n",
    "        position_ids = position_ids.clone()\n",
    "\n",
    "    if reset_position_ids or reset_attention_mask:\n",
    "        # Loop through the batches:\n",
    "        for b in range(batch_size):\n",
    "\n",
    "            # Find indecies where EOD token is.\n",
    "            eod_index = position_ids[b, data[b] == eod_token]\n",
    "            # Detach indecies from positions if going to modify positions.\n",
    "            if reset_position_ids:\n",
    "                eod_index = eod_index.clone()\n",
    "\n",
    "            # Loop through EOD indecies:\n",
    "            prev_index = 0\n",
    "            for j in range(eod_index.size()[0]):\n",
    "                i = eod_index[j]\n",
    "                # Mask attention loss.\n",
    "                if reset_attention_mask:\n",
    "                    attention_mask[b, 0, (i+1):, :(i+1)] = 0\n",
    "                # Reset positions.\n",
    "                if reset_position_ids:\n",
    "                    position_ids[b, (i+1):] -= (i + 1 - prev_index)\n",
    "                    prev_index = i + 1\n",
    "\n",
    "    return attention_mask, loss_mask, position_ids\n",
    "\n",
    "\n",
    "def get_batch(context_tokens, tokenizer, batchsize):\n",
    "    tokens = context_tokens\n",
    "    tokens = tokens.view(batchsize, -1).contiguous()\n",
    "\n",
    "    # Get the masks and postition ids.\n",
    "    attention_mask, loss_mask, position_ids = get_masks_and_position_ids(\n",
    "        tokens,\n",
    "        tokenizer.encoder['<eod>'],\n",
    "        reset_position_ids=True,\n",
    "        reset_attention_mask=True)\n",
    "\n",
    "    return tokens, attention_mask, position_ids\n",
    "\n",
    "def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    " \n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    if top_p > 0.0:\n",
    "        #convert to 1D\n",
    "        logits=logits.view(logits.size()[1]).contiguous()\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        #going back to 2D\n",
    "        logits=logits.view(1, -1).contiguous()\n",
    "\t\n",
    "    return logits\n",
    "\n",
    "\n",
    "def load_tnews_data(data_path, data_type, tokenizer, few_shot=False, seq_length=1024):\n",
    "    # args = get_args()\n",
    "\n",
    "    filename = os.path.join(data_path, data_type+'.json')\n",
    "    objs = []\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            objs.append(json.loads(line.strip()))\n",
    "\n",
    "    pad_id = tokenizer.encoder['<pad>']\n",
    "    # args.eod_token = tokenizer.encoder['<eod>']\n",
    "\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_reverse = {}\n",
    "    with open(os.path.join(data_path, 'labels.json')) as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            obj = json.loads(line.strip())\n",
    "            labels.append(obj['label_desc'])\n",
    "            label_map[obj['label_desc']] = i\n",
    "            label_reverse[obj['label']] = obj['label_desc']\n",
    "\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_labels = []\n",
    "    for _, obj in enumerate(objs):\n",
    "        sentence = obj['sentence']\n",
    "        tokenized_sentence = tokenizer.encode(sentence)[:seq_length-20]\n",
    "        obj['label_desc'] = label_reverse[obj['label']]\n",
    "\n",
    "        if few_shot:\n",
    "            cur_labels = random.sample(labels, 3)\n",
    "            while obj['label_desc'] in cur_labels:\n",
    "                cur_labels = random.sample(labels, 3)\n",
    "            cur_labels.append(obj['label_desc'])\n",
    "            cur_label = cur_labels.index(obj['label_desc'])\n",
    "            assert cur_label != -1\n",
    "        else:\n",
    "            cur_labels = labels\n",
    "            cur_label = label_map[obj['label_desc']]\n",
    "\n",
    "        all_labels.append(cur_label)\n",
    "\n",
    "        for _, label in enumerate(cur_labels):\n",
    "            prompt = \"这是关于{}的文章：\".format(label)\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "            prompt_len = len(prompt_tokens)\n",
    "            tokens = prompt_tokens + tokenized_sentence\n",
    "            second_mask = [0] * (seq_length-1)\n",
    "            for idx in range(prompt_len-1, len(tokens)-1):\n",
    "                second_mask[idx] = 1\n",
    "            all_masks.append(second_mask)\n",
    "            token_length = len(tokens)\n",
    "            assert token_length < seq_length\n",
    "            tokens.extend([pad_id] * (seq_length - token_length))\n",
    "            all_tokens.append(tokens)\n",
    "    \n",
    "    all_tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "    all_masks = torch.tensor(all_masks, dtype=torch.float)\n",
    "    dataset = TensorDataset(all_tokens, all_masks)\n",
    "\n",
    "    if data_type == 'train':\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    # Torch dataloader.\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       sampler=sampler,\n",
    "                                       num_workers=0,\n",
    "                                       pin_memory=True), all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 每一条新闻标题，通过交叉熵损失，在15个label下有各自的相关性损失。argin看看哪个损失最小\n",
    "def eval(model, train_dataloader, all_labels, optimizer, device, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = []\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            tokens, masks = [x.to(device) for x in batch]\n",
    "            tokens, attention_mask, position_ids = get_batch(tokens, tokenizer, batchsize=4)\n",
    "            output = model(tokens)\n",
    "\n",
    "            losses = loss_fcn(output[:, :-1, :].contiguous().float().reshape(-1,30000), tokens[:, 1:].reshape(-1))\n",
    "            # output = torch.sum(losses * masks, 1) / torch.sum(masks, -1)\n",
    "            \n",
    "            res.append(losses)\n",
    "        \n",
    "    cnt = 0\n",
    "    label_size = 15\n",
    "    num_inst = len(res) // label_size\n",
    "    for x in range(num_inst):\n",
    "        label = all_labels[x]\n",
    "        cur_res = res[x*label_size:(x+1)*label_size]\n",
    "        pos = np.argmin(torch.tensor(cur_res).numpy())\n",
    "        if pos == label:\n",
    "            cnt += 1\n",
    "    print('准确率', float(cnt)/num_inst)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, all_labels, optimizer, device, tokenizer):\n",
    "    model.train()\n",
    "\n",
    "    train_dataloader = tqdm(train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        tokens, masks = [x.to(device) for x in batch]\n",
    "        tokens, attention_mask, position_ids = get_batch(tokens, tokenizer, batchsize=4)\n",
    "        output = model(tokens)\n",
    "        losses = loss_fcn(output[:, :-1, :].contiguous().float().reshape(-1,30000), tokens[:, 1:].reshape(-1))  # 分批次，参考fintune\n",
    "        losses.backward()\n",
    "        train_dataloader.set_description('train loss: %s' % losses.item())\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, train_labels = load_tnews_data('../nlpdata/tnews_public', 'dev_small', tokenizer)\n",
    "\n",
    "import transformers\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=1.5e-6, eps=1.0e-9)  # lr=1.5e-4, eps=1.0e-9\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "loss_fcn.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.32117268443107605: 100%|██████████| 240/240 [01:12<00:00,  3.30it/s]\n",
      "train loss: 0.2840106785297394: 100%|██████████| 240/240 [01:12<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(model, train_dataloader, train_labels, optimizer, device, tokenizer)\n",
    "    torch.save(model.state_dict(), \"trained.pth\")  # 只保存模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:20<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.25\n"
     ]
    }
   ],
   "source": [
    "eval(model, train_dataloader, train_labels, optimizer, device, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保险好。\n"
     ]
    }
   ],
   "source": [
    "ask_question('保险好吗？', max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "集训,回来后,我就开始画了,当时画的是\n"
     ]
    }
   ],
   "source": [
    "sample('去北京画室参加', max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Model(\n",
    "    vocab_size=30000,\n",
    "    layer_size=12,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=768,\n",
    "    num_attention_heads=12,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)\n",
    "\n",
    "state_dict = torch.load('trained.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5516efe75593d06eaf40bae3490a069baebb94ad14a4bb81cab04a47f7ae72c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cpm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
