{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from GPT2 import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' #'cuda'\n",
    "\n",
    "model = GPT2Model(\n",
    "    vocab_size=30000,\n",
    "    layer_size=12,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=768,\n",
    "    num_attention_heads=12,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)\n",
    "\n",
    "state_dict = torch.load('save_distill.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    'GPT2/bpe/vocab.json',\n",
    "    'GPT2/bpe/chinese_vocab.model',\n",
    "    max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(text, max_len=10):\n",
    "    ids = tokenizer.encode(text)\n",
    "    input_id = torch.tensor((np.array(ids).reshape(1, -1).astype('int64'))).to(device)\n",
    "    output, cached_kvs = model(input_id, use_cache=True)\n",
    "    nid = int(np.argmax(output[0, -1].detach().cpu().numpy()))\n",
    "    ids += [nid]\n",
    "    out = [nid]\n",
    "    for i in range(max_len):\n",
    "        input_id = torch.tensor(np.array([nid]).reshape(1, -1).astype('int64')).to(device)\n",
    "        output, cached_kvs = model(input_id, cached_kvs, use_cache=True)\n",
    "        nid = int(np.argmax(output[0, -1].detach().cpu().numpy()))\n",
    "        ids += [nid]\n",
    "        if nid==3:\n",
    "            break\n",
    "        out.append(nid)\n",
    "    print(tokenizer.decode(out))\n",
    "\n",
    "def ask_question(question, max_len=10):\n",
    "    sample('''问题：中国的首都是哪里？\n",
    "    答案：北京。\n",
    "    问题：李白在哪个朝代？\n",
    "    答案：唐朝。\n",
    "    问题：%s\n",
    "    答案：''' % question, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.samplers import DistributedBatchSampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, split, tokenizer: GPT2Tokenizer, seq_length=1024, ratio=1):\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ratio = ratio\n",
    "\n",
    "        self.pad_id = tokenizer.encoder['<pad>']\n",
    "        self.eod_token = tokenizer.encoder['<eod>']\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        self.samples = self.process(data)\n",
    "        \n",
    "\n",
    "    def process(self, data):\n",
    "        samples = []\n",
    "        for doc in tqdm(data[:int(self.ratio * len(data))]):\n",
    "            token_ids = self.tokenizer.encode(doc)\n",
    "            token_ids.append(self.eod_token)\n",
    "            start = 0\n",
    "            while start + self.seq_length + 1 < len(token_ids):\n",
    "                samples.append(token_ids[start: start + self.seq_length + 1])\n",
    "                start = start + self.seq_length + 1\n",
    "            samples.append(token_ids[start:] + [self.pad_id] * (self.seq_length + 1 - (len(token_ids) - start)))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "    def collate(self, samps):\n",
    "        bs = len(samps)\n",
    "\n",
    "        # # triangle attention mask\n",
    "        # attn_mask = torch.tril(torch.ones((self.seq_length, self.seq_length))).unsqueeze(0)\n",
    "        # position_ids = torch.arange(self.seq_length, dtype=torch.long).unsqueeze(0).repeat(bs, 1)\n",
    "\n",
    "        # the data that need to go through the model\n",
    "        batch_sample = {\n",
    "            \"input_ids\": torch.ones(bs, self.seq_length).long() * self.pad_id,\n",
    "            # \"attention_mask\": attn_mask.unsqueeze(1),\n",
    "            # \"position_ids\": position_ids,\n",
    "        }\n",
    "\n",
    "        # the data that do not need to go through the model\n",
    "        no_model_sample = {\n",
    "            \"labels\": torch.ones(bs, self.seq_length).long() * self.pad_id,\n",
    "            \"loss_mask\": torch.zeros(bs, self.seq_length).float()\n",
    "        }\n",
    "\n",
    "        for i, samp in enumerate(samps):\n",
    "            assert len(samp) == self.seq_length + 1, (len(samp), self.seq_length)\n",
    "            batch_sample[\"input_ids\"][i] = torch.tensor(samp[:-1], dtype=torch.long)\n",
    "            no_model_sample[\"labels\"][i] = torch.tensor(samp[1:], dtype=torch.long)\n",
    "            no_model_sample[\"loss_mask\"][i] = (no_model_sample[\"labels\"][i] != self.pad_id).float()\n",
    "\n",
    "        return batch_sample, no_model_sample\n",
    "\n",
    "\n",
    "def load_data(data_path, data_type, tokenizer, ratio=1):\n",
    "\n",
    "    # Dataset\n",
    "    filename = os.path.join(data_path, data_type + '.txt')\n",
    "    dataset = GenDataset(filename, data_type, tokenizer, ratio=ratio)\n",
    "    \n",
    "    # Use a random sampler with distributed batch sampler.\n",
    "    if data_type == 'train':\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    # Torch dataloader.\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       sampler=sampler,\n",
    "                                       num_workers=0,\n",
    "                                       pin_memory=True,\n",
    "                                       collate_fn=dataset.collate), dataset\n",
    "                                       \n",
    "\n",
    "def evaluate(model, dataloader, device, mode=\"dev\"):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, no_model_batch in tqdm(dataloader):\n",
    "            for k in batch.keys():\n",
    "                batch[k] = batch[k].to(device)\n",
    "            for k in no_model_batch.keys():\n",
    "                no_model_batch[k] = no_model_batch[k].to(device)\n",
    "\n",
    "            output = model(batch['input_ids'])\n",
    "            labels = no_model_batch[\"labels\"]\n",
    "\n",
    "            # cross_entropy loss\n",
    "            losses = loss_fcn(output.contiguous().float().reshape(-1,30000), labels.reshape(-1))\n",
    "            loss_mask = no_model_batch[\"loss_mask\"]\n",
    "                \n",
    "            loss = torch.sum(losses * loss_mask, dim=-1) / loss_mask.sum(dim=-1)\n",
    "\n",
    "            all_losses.extend(loss.tolist())\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device, mode=\"train\"):\n",
    "    model.train()\n",
    "    for batch, no_model_batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device)\n",
    "        for k in no_model_batch.keys():\n",
    "            no_model_batch[k] = no_model_batch[k].to(device)\n",
    "        output = model(batch['input_ids'])\n",
    "        labels = no_model_batch[\"labels\"]\n",
    "        losses = loss_fcn(output.contiguous().float().reshape(-1,30000), labels.reshape(-1))\n",
    "        loss_mask = no_model_batch[\"loss_mask\"]\n",
    "        loss = torch.sum(losses * loss_mask, dim=-1) / loss_mask.sum(dim=-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21161/21161 [00:10<00:00, 2016.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, dataset = load_data('../nlpdata/data/STC', 'train', tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "loss_fcn.to(device)\n",
    "\n",
    "import transformers\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=1.5e-6, eps=1.0e-9)  # lr=1.5e-4, eps=1.0e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_dataloader, device, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21161/21161 [2:32:57<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_dialog.pth\")  # 只保存模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from GPT2 import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' #'cuda'\n",
    "\n",
    "model = GPT2Model(\n",
    "    vocab_size=30000,\n",
    "    layer_size=12,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=768,\n",
    "    num_attention_heads=12,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)\n",
    "\n",
    "state_dict = torch.load('trained_dialog.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    'GPT2/bpe/vocab.json',\n",
    "    'GPT2/bpe/chinese_vocab.model',\n",
    "    max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.593 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question('保险', max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我觉得,还是买保险吧,因为我是老百姓,我的钱是我的,我的钱是我的,我的钱是我的,我的钱是我的,\n"
     ]
    }
   ],
   "source": [
    "sample('对话上文:老年人该买保险吗？ 回复:', max_len=50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5516efe75593d06eaf40bae3490a069baebb94ad14a4bb81cab04a47f7ae72c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cpm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
