{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../dataset/THUCNews/彩票/256822.txt'\n",
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str = str.replace(' ', '').replace('\\n\\u3000\\u3000', '')\n",
    "print(new_str)\n",
    "print(len(new_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2 import GPT2Model, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    'GPT2/bpe/vocab.json',\n",
    "    'GPT2/bpe/chinese_vocab.model',\n",
    "    max_len=512)\n",
    "tokenized_str = tokenizer.encode(new_str)\n",
    "print(tokenized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_eng = \"This is a good example of training GPT2 to solve this problem\"\n",
    "tokenized_str = tokenizer.encode(str_eng)\n",
    "print(tokenized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_folders(path):\n",
    "    folders = []\n",
    "    g = os.walk(path)  \n",
    "    for path,dir_list,file_list in g:  \n",
    "        for dir_name in dir_list:\n",
    "            folders.append(dir_name)\n",
    "    return folders\n",
    "def get_files(path):\n",
    "    files = []\n",
    "    g = os.walk(path)  \n",
    "    for path,dir_list,file_list in g:  \n",
    "        for file_name in file_list:\n",
    "            files.append(file_name)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    每个类别放到一个txt里\n",
    "'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = '../dataset/THUCNews'\n",
    "save_path = '../dataset/THUCNews_processed'\n",
    "folders = get_folders(path)\n",
    "print(folders)\n",
    "for folder in folders:\n",
    "    save_list = []\n",
    "    files = get_files(os.path.join(path, folder))\n",
    "    for file in tqdm(files):\n",
    "        with open(os.path.join(path, folder, file),\"r\", encoding='utf-8') as f:\n",
    "            str = f.read()\n",
    "            new_str = str.replace(' ', '').replace('\\n\\u3000\\u3000', '')\n",
    "            save_list.append(new_str)\n",
    "    with open(os.path.join(save_path, folder + '.txt'),\"w+\", encoding='utf-8') as f:\n",
    "        for line in save_list:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    二分类json数据集\n",
    "'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "path = '../dataset/THUCNews'\n",
    "save_path = '../dataset/THUCNews_processed'\n",
    "folders = get_folders(path)\n",
    "print(folders)\n",
    "\n",
    "objs_financial = []\n",
    "objs_others = []\n",
    "\n",
    "for folder in folders:\n",
    "    if folder in ['财经', '彩票', '股票']:\n",
    "        files = get_files(os.path.join(path, folder))\n",
    "        for file in tqdm(files):\n",
    "            with open(os.path.join(path, folder, file),\"r\", encoding='utf-8') as f:\n",
    "                obj = dict()\n",
    "                str = f.read()\n",
    "                new_str = str.replace(' ', '').replace('\\n\\u3000\\u3000', '')\n",
    "                obj['label_desc'] = 'news_finance'\n",
    "                obj['sentence'] = new_str\n",
    "                objs_financial.append(obj)\n",
    "    else:\n",
    "        label = 'news_' + folder\n",
    "        files = get_files(os.path.join(path, folder))\n",
    "        for file in tqdm(files):\n",
    "            with open(os.path.join(path, folder, file),\"r\", encoding='utf-8') as f:\n",
    "                obj = dict()\n",
    "                str = f.read()\n",
    "                new_str = str.replace(' ', '').replace('\\n\\u3000\\u3000', '')\n",
    "                obj['label_desc'] = label\n",
    "                obj['sentence'] = new_str\n",
    "                objs_others.append(obj)\n",
    "\n",
    "random.shuffle(objs_financial)\n",
    "random.shuffle(objs_others)\n",
    "\n",
    "num = min(len(objs_financial), len(objs_others))\n",
    "objs = []\n",
    "objs.extend(objs_financial[:num])\n",
    "objs.extend(objs_others[:num])\n",
    "\n",
    "random.shuffle(objs)\n",
    "\n",
    "num = int(len(objs)*0.9)\n",
    "objs_train = objs[:num]\n",
    "objs_test = objs[num:]\n",
    "\n",
    "filename = '../dataset/THUCNews_processed/train_financial.json'\n",
    "with open(filename, 'w', encoding='utf-8') as fout:\n",
    "    for obj in objs_train:\n",
    "        line = json.dumps(obj, ensure_ascii=False)\n",
    "        fout.write(line)\n",
    "        fout.write('\\n')\n",
    "\n",
    "filename = '../dataset/THUCNews_processed/test_financial.json'\n",
    "with open(filename, 'w', encoding='utf-8') as fout:\n",
    "    for obj in objs_test:\n",
    "        line = json.dumps(obj, ensure_ascii=False)\n",
    "        fout.write(line)\n",
    "        fout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf426626c1747c2231e550496924bf3d6568132865084069abb416574bffa72a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cpm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
